# -*- coding: utf-8 -*-
"""nlp_adampamungkas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mf8hE0td43REb5q4tOInjNEnlTlTO8T0
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import re

df_true = pd.read_csv('True.csv')
df_fake = pd.read_csv('Fake.csv')

print("Total data True = ", len(df_true))
print('total data fake = ', len(df_fake))
df_true

df_true.drop(columns=(['subject', 'date']), inplace=True)
df_fake.drop(columns=(['subject', 'date']), inplace=True)

df_true['label'] = 0
df_fake['label'] = 1

df_true.head(1)

df_fake.head(1)

pattern = "^[A-Z/]+ \(Reuters\) - "

sum([1 for text in df_true["text"] if not re.match(pattern, text)])

def clean(text: str) -> str:
    return re.sub(pattern, "", text)

df_true['text'] = df_true['text'].apply(clean)
df_fake['text'] = df_fake['text'].apply(clean)
df

df = pd.concat([df_fake, df_true])
df

x_title = df[['title','text']]
y_label = df['label']
features_dict = {name: np.array(value) for name, value in x_title.items()}

from sklearn.model_selection import train_test_split

x_title = df[['title','text']]
y_label = df['label']
x_train, x_test, y_train, y_test = train_test_split(x_title, y_label, test_size=0.2)

def get_output_sequence_length(column):
    lengths = [len(x.split()) for x in column]
    return max(lengths)

inputs = {}
for name, column in x_title.items():    
    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=tf.string)
    
outputs = []
for name, input in inputs.items():
    output_sequence_length = get_output_sequence_length(x_title[name])
    text_vectorizer = tf.keras.layers.TextVectorization(output_sequence_length=output_sequence_length)
    text_vectorizer.adapt(x_title[name])
    
    x = text_vectorizer(input)
    outputs.append(x)

!wget --no-check-certificate \
     http://nlp.stanford.edu/data/glove.6B.zip \
     -O /tmp/glove.6B.zip

import zipfile, os
with zipfile.ZipFile("/tmp/glove.6B.zip", "r") as zip_ref:
    zip_ref.extractall("/tmp/glove")

embeddings_index = {}
f = open("/tmp/glove/glove.6B.100d.txt")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype="float32")
    embeddings_index[word] = coefs
f.close()

print(f"Found {len(embeddings_index)} word vectors.")

embedding_matrix = np.zeros((text_vectorizer.vocabulary_size(), 100))
for i, word in enumerate(text_vectorizer.get_vocabulary()):
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

# Tokenizer mengubah kalimat kedalam numeric
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(x_train)
tokenizer.fit_on_texts(x_test)

sekuens_latih = tokenizer.texts_to_sequences(x_train)
sekuens_test = tokenizer.texts_to_sequences(x_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=text_vectorizer.vocabulary_size(), output_dim=100, weights=[embedding_matrix], input_length=100, trainable=False),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    # tf.keras.layers.Dense(64, activation='relu'),
    # tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

callbacks = [tf.keras.callbacks.EarlyStopping(patience=10)]

model.fit(padded_train,
          y_label, 
          epochs=30, 
          validation_split=0.25,
          batch_size=512,
          verbose=2,
          )

